# -*- coding: utf-8 -*-
"""ML-AM_Proposed_implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18VPKl4Xn_WT_cHRbbkv4Ohs5cIm56GdV
"""

import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from sklearn.metrics import accuracy_score
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras import backend as K
from tensorflow.keras.losses import BinaryFocalCrossentropy
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.model_selection import train_test_split

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2

from google.colab import drive
drive.mount("/content/drive", force_remount=True)
import os
GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'ML for AM (additive manufacturing)/proposedImplementation/new_data'
GOOGLE_DRIVE_PATH = os.path.join('drive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print(os.listdir(GOOGLE_DRIVE_PATH))

# Commented out IPython magic to ensure Python compatibility.
plt.rcParams['figure.figsize'] = (10.0, 8.0)
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

# %matplotlib inline

def gradient(grad):
    grad['gradient'] = grad['height'].diff().fillna(0)
    return grad

"""Data Processing, uses first 7 datasets as train data, and last 6 as test data"""

train_dfs = []
test_dfs = []
for i in range(1, 8):
    train_data = pd.read_csv(f'/content/drive/MyDrive/ML for AM (additive manufacturing)/proposedImplementation/new_data/dataset{i}.csv')
    train_data = gradient(train_data)
    train_dfs.append(train_data)

for i in range(8, 16):
    test_data = pd.read_csv(f'/content/drive/MyDrive/ML for AM (additive manufacturing)/proposedImplementation/new_data/dataset{i}.csv')
    test_data = gradient(train_data)
    test_dfs.append(test_data)
train_data = pd.concat(train_dfs, ignore_index=True)
test_data = pd.concat(test_dfs, ignore_index=True)

X_train_np = train_data['gradient'].values.reshape(-1, 1)
y_train_np = train_data['pore'].values.reshape(-1, 1)
X_test_np = test_data['gradient'].values.reshape(-1, 1)
y_test_np = test_data['pore'].values.reshape(-1, 1)

X_train = tf.convert_to_tensor(X_train_np, dtype=tf.float32)
y_train = tf.convert_to_tensor(y_train_np, dtype=tf.float32)
X_test = tf.convert_to_tensor(X_test_np, dtype=tf.float32)
y_test = tf.convert_to_tensor(y_test_np, dtype=tf.float32)

"""Train Data: 36 Instances of pore formation, 1199 instances of no pore formation"""

print(np.count_nonzero(y_train == 0))
print(np.count_nonzero(y_train == 1))

print(1199/36)

print(np.count_nonzero(y_test == 0))
print(np.count_nonzero(y_test == 1))

"""Test Data: 42 Instances of pore formation, 486 instances of no pore formation"""

# Not being used
def focal_loss(y_true, y_pred, alpha=0.25, gamma=2):
    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())
    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))
    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))
    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))

"""Implementation 1 w/ weighted classes and dropout"""

#Implementation 1
# Calculate class weights
#class_weights = compute_class_weight('balanced', classes=np.unique(y_train_np), y=y_train_np.flatten())
#class_weights_dict = dict(enumerate(class_weights))

class_weights = {0: 1, 1: 30}

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu',bias_regularizer='L2', input_shape=(1,)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64, activation='relu',bias_regularizer='L2'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32, activation='relu',bias_regularizer='L2'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(16, activation='relu',bias_regularizer='L2'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=50, batch_size=32, class_weight=class_weights, validation_split=0.33)
loss, accuracy = model.evaluate(X_test, y_test)

y_pred_proba = model.predict(X_test)
y_pred = np.round(y_pred_proba).astype(int)

print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

matrix = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=['No pore formation', 'Pore formation'])
disp.plot(cmap='Blues')
plt.title(f'Pore Formation Prediction Results')
plt.show()

print(classification_report(y_test, y_pred))

c_idx = np.where((y_test == 1) & (y_test == y_pred))

correct_pores = X_test[c_idx]

print("Gradient values of correctly classified pores:")
print(correct_pores)

m_idx = np.where((y_test == 1) & (y_test != y_pred))

misclassified_pores = X_test[m_idx]

print("Gradient values of misclassified pores:")
print(misclassified_pores)

"""Implementation 2 with no weighted classes, no dropout, but oversampling minority class and using BinaryFocalCrossEntropy loss that penalizes missed minority class more than missing majority class"""

#Conducts oversampling
from imblearn.over_sampling import RandomOverSampler
oversampler = RandomOverSampler(sampling_strategy=0.55)
X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_np, y_train_np)

X_train_over = tf.convert_to_tensor(X_train_resampled, dtype=tf.float32)
y_train_over = tf.convert_to_tensor(y_train_resampled, dtype=tf.float32)
print(np.count_nonzero(y_train_over == 0))
print(np.count_nonzero(y_train_over == 1))

"""Train Data: 659 Instances of pore formation, 1199 instances of no pore formation"""

#Implementation 2
class_weights = {0: 1, 1: 10}

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(1,)),
    #tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64, activation='relu'),
    #tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32, activation='relu'),
    #tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(16, activation='relu'),
    #tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss=BinaryFocalCrossentropy(), metrics=['accuracy'])
model.fit(X_train_over, y_train_over, epochs=50, batch_size=32, class_weight=class_weights, validation_split=0.33)
loss_2, accuracy_2 = model.evaluate(X_test, y_test)

y_pred_2 = model.predict(X_test)
y_pred_2 = np.round(y_pred_2).astype(int)

print("Test Loss:", loss_2)
print("Test Accuracy:", accuracy_2)

matrix_2 = confusion_matrix(y_test, y_pred_2)
disp = ConfusionMatrixDisplay(confusion_matrix=matrix_2, display_labels=['No pore formation', 'Pore formation'])
disp.plot(cmap='Blues')
plt.title(f'Pore Formation Prediction Results')
plt.show()

print(classification_report(y_test, y_pred_2))

"""Implementation 3 using original data w/ a decision tree classifier"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, accuracy_score

clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)

y_pred_3 = clf.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred_3)
print("Test Accuracy:", test_accuracy)

matrix_3 = confusion_matrix(y_test, y_pred_3)
disp = ConfusionMatrixDisplay(confusion_matrix=matrix_3, display_labels=['No pore formation', 'Pore formation'])
disp.plot(cmap='Blues')
plt.title(f'Pore Formation Prediction Results')
plt.show()